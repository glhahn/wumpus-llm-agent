{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hunt the Wumpus LLM Agent Evaluation\n",
    "\n",
    "This notebook evaluates the performance of different Hunt the Wumpus LLM agent prompts by comparing:\n",
    "- Game outcomes and win rates\n",
    "- Performance metrics (turns, rooms explored, arrow usage)\n",
    "- Error rates and response times\n",
    "\n",
    "Two trials are analyzed:\n",
    "1. Basic Action Prompt\n",
    "    * initial action prompt with the strategy and basic guidelines.\n",
    "2. Enhanced Action Prompt + System Message \n",
    "    * current action prompt with more detailed considerations/guidelines and added system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# set style for visualizations - using a built-in style\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "# plot parameters\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the game data from CSV and prepare it for analysis. Each CSV contains a trial of ~30 game runs, with each row representing the following metrics from one game:\n",
    "\n",
    "* timestamp of game completion\n",
    "* number of turns taken\n",
    "* number of rooms explored\n",
    "* boolean flags for different loss conditions (death by pit, Wumpus, or running out of arrows)\n",
    "* boolean for game won\n",
    "* arrows remaining\n",
    "* the number of errors associated with generating a turn action (i.e., Pydantic structured output validation errors)\n",
    "* average and total LLM response times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Current directory: /Users/ghahn/Projects/wumpus-llm-agent\n",
       "Files in directory: []\n",
       "\n",
       "Total games analyzed: 64\n",
       "\n",
       "Games per trial:\n",
       "trial_group\n",
       "Basic Action Prompt                        32\n",
       "Enhanced Action Prompt + System Message    32\n",
       "Name: count, dtype: int64\n",
       "\n",
       "First 5 rows of Basic Action Prompt trial:\n",
       "                   timestamp  num_turns  rooms_explored  death_by_pit  death_by_wumpus  death_by_arrows  game_won  arrows_remaining  action_generation_errors  average_response_time\n",
       "0 2024-09-26 23:31:59.223440          2               1             0                0                0         0                 5                       NaN              14.486365\n",
       "1 2024-09-26 23:31:31.407691          3               3             1                0                0         0                 5                       NaN               8.195913\n",
       "2 2024-09-26 23:30:43.749341          3               2             0                0                0         0                 5                       NaN               9.629337\n",
       "3 2024-09-26 23:29:48.110002          7               7             1                0                0         0                 5                       NaN               7.506790\n",
       "4 2024-09-26 23:29:05.794541          2               1             0                0                0         0                 5                       NaN              14.425337\n",
       "\n",
       "First 5 rows of Enhanced Action Prompt + System Message trial:\n",
       "                    timestamp  num_turns  rooms_explored  death_by_pit  death_by_wumpus  death_by_arrows  game_won  arrows_remaining  action_generation_errors  average_response_time\n",
       "32 2024-09-28 15:47:47.424979         13              10             0                0                0         0                 3                       1.0               5.260955\n",
       "33 2024-09-28 15:47:05.703040          4               3             0                0                0         0                 5                       1.0               7.527796\n",
       "34 2024-09-28 15:46:48.383623          1               0             0                0                0         1                 4                       0.0              14.121232\n",
       "35 2024-09-28 15:46:26.419940          2               1             0                0                0         1                 4                       0.0               9.363225\n",
       "36 2024-09-28 15:45:58.554920          3               3             1                0                0         0                 5                       0.0               8.207802\n",
       "\n",
       "Summary statistics for numerical columns:\n",
       "       num_turns  rooms_explored  arrows_remaining  action_generation_errors  average_response_time\n",
       "count  64.000000       64.000000         64.000000                 32.000000              64.000000\n",
       "mean    4.734375        3.468750          4.109375                  0.312500               8.039497\n",
       "std     3.859608        3.018297          1.223022                  0.470929               4.161904\n",
       "min     1.000000        0.000000         -1.000000                  0.000000               0.000000\n",
       "25%     2.000000        1.750000          4.000000                  0.000000               6.015440\n",
       "50%     4.000000        3.000000          4.000000                  0.000000               7.578805\n",
       "75%     6.000000        4.000000          5.000000                  1.000000               9.417786\n",
       "max    19.000000       13.000000          5.000000                  1.000000              27.193654\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# configure pandas display options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# get the directory containing the CSV files\n",
    "notebook_path = Path.cwd()\n",
    "print(\"Current directory:\", notebook_path)\n",
    "print(\"Files in directory:\", list(notebook_path.glob(\"*.csv\")))\n",
    "\n",
    "\n",
    "def load_wumpus_data(trial1_path, trial2_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the Wumpus game data from separate trial CSV files.\n",
    "    \"\"\"\n",
    "    # load both trials\n",
    "    df1 = pd.read_csv(trial1_path, parse_dates=[\"timestamp\"])\n",
    "    df2 = pd.read_csv(trial2_path, parse_dates=[\"timestamp\"])\n",
    "\n",
    "    # add trial identifiers\n",
    "    df1[\"trial_group\"] = \"Basic Action Prompt\"\n",
    "    df2[\"trial_group\"] = \"Enhanced Action Prompt + System Message\"\n",
    "\n",
    "    # combine the dataframes\n",
    "    df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# load the data using paths relative to notebook_path\n",
    "trial1_path = notebook_path / \"game_metrics_detailed_trial1.csv\"\n",
    "trial2_path = notebook_path / \"game_metrics_detailed_trial2.csv\"\n",
    "\n",
    "df = load_wumpus_data(trial1_path, trial2_path)\n",
    "\n",
    "print(\"\\nTotal games analyzed:\", len(df))\n",
    "print(\"\\nGames per trial:\")\n",
    "print(df[\"trial_group\"].value_counts())\n",
    "\n",
    "# show first 5 rows of each trial\n",
    "columns_to_display = [\n",
    "    \"timestamp\",\n",
    "    \"num_turns\",\n",
    "    \"rooms_explored\",\n",
    "    \"death_by_pit\",\n",
    "    \"death_by_wumpus\",\n",
    "    \"death_by_arrows\",\n",
    "    \"game_won\",\n",
    "    \"arrows_remaining\",\n",
    "    \"action_generation_errors\",\n",
    "    \"average_response_time\",\n",
    "]\n",
    "\n",
    "print(\"\\nFirst 5 rows of Basic Action Prompt trial:\")\n",
    "print(df[df[\"trial_group\"] == \"Basic Action Prompt\"][columns_to_display].head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of Enhanced Action Prompt + System Message trial:\")\n",
    "print(\n",
    "    df[df[\"trial_group\"] == \"Enhanced Action Prompt + System Message\"][\n",
    "        columns_to_display\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "# print summary statistics for key metrics\n",
    "print(\"\\nSummary statistics for numerical columns:\")\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"num_turns\",\n",
    "            \"rooms_explored\",\n",
    "            \"arrows_remaining\",\n",
    "            \"action_generation_errors\",\n",
    "            \"average_response_time\",\n",
    "        ]\n",
    "    ].describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Outcome Analysis (Wins)\n",
    "\n",
    "Create heatmaps showing game outcomes for each trial, with color intensity based on the number of turns taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_outcome_heatmap(df, trial_name):\n",
    "    \"\"\"\n",
    "    Create a 6x5 heatmap showing game outcomes with turn count intensity.\n",
    "    \"\"\"\n",
    "    trial_data = df[df[\"trial_group\"] == trial_name].reset_index()\n",
    "\n",
    "    # create 6x5 matrix of game outcomes\n",
    "    outcomes = np.zeros((6, 5))\n",
    "    turns = np.zeros((6, 5))\n",
    "\n",
    "    for i in range(30):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        outcomes[row, col] = trial_data.loc[i, \"game_won\"]\n",
    "        turns[row, col] = trial_data.loc[i, \"num_turns\"]\n",
    "\n",
    "    # normalize turn counts for color intensity\n",
    "    turns_normalized = (turns - turns.min()) / (turns.max() - turns.min())\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # create custom colormap: red for losses, green for wins\n",
    "    colors = np.zeros((6, 5, 3))\n",
    "    colors[outcomes == 0] = [1, 0, 0]  # red for losses\n",
    "    colors[outcomes == 1] = [0, 1, 0]  # green for wins\n",
    "\n",
    "    # adjust color intensity based on turn count\n",
    "    for i in range(3):\n",
    "        colors[:, :, i] = colors[:, :, i] * (1 - turns_normalized * 0.5)\n",
    "\n",
    "    plt.imshow(colors)\n",
    "\n",
    "    # add turn count text to each cell\n",
    "    for i in range(6):\n",
    "        for j in range(5):\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{int(turns[i, j])}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if turns_normalized[i, j] > 0.5 else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Game Outcomes - {trial_name}\\nCell colors: Green=Win, Red=Loss\\nNumbers show turn count\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "# create heatmaps for each trial\n",
    "for trial in df[\"trial_group\"].unique():\n",
    "    outcome_heatmap = create_outcome_heatmap(df, trial)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Analysis\n",
    "\n",
    "Calculate and compare key performance metrics between trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(df, trial_name):\n",
    "    \"\"\"\n",
    "    Calculate key performance metrics for a trial.\n",
    "    \"\"\"\n",
    "    trial_data = df[df[\"trial_group\"] == trial_name]\n",
    "\n",
    "    metrics = {\n",
    "        \"Total Games\": len(trial_data),\n",
    "        \"Win Rate\": (trial_data[\"game_won\"].mean() * 100),\n",
    "        \"Avg Turns per Game\": trial_data[\"num_turns\"].mean(),\n",
    "        \"Avg Turns per Win\": trial_data[trial_data[\"game_won\"] == 1][\n",
    "            \"num_turns\"\n",
    "        ].mean(),\n",
    "        \"Avg Rooms Explored\": trial_data[\"rooms_explored\"].mean(),\n",
    "        \"Arrow Efficiency\": (\n",
    "            5 - trial_data[trial_data[\"game_won\"] == 1][\"arrows_remaining\"].mean()\n",
    "        ),  # average number of arrows to achieve win\n",
    "        \"Death by Pit %\": (trial_data[\"death_by_pit\"].mean() * 100),\n",
    "        \"Death by Wumpus %\": (trial_data[\"death_by_wumpus\"].mean() * 100),\n",
    "        \"Death by Arrows %\": (trial_data[\"death_by_arrows\"].mean() * 100),\n",
    "        \"Avg Errors per Game\": trial_data[\"action_generation_errors\"].mean(), # this metric was added after the first trial\n",
    "        \"Avg Response Time\": trial_data[\"average_response_time\"].mean(),\n",
    "    }\n",
    "\n",
    "    return pd.Series(metrics)\n",
    "\n",
    "\n",
    "# calculate and display metrics\n",
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        trial: calculate_performance_metrics(df, trial)\n",
    "        for trial in df[\"trial_group\"].unique()\n",
    "    }\n",
    ")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Time Analysis\n",
    "\n",
    "Analyze response times and their relationship with game outcomes and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_response_time_analysis(df):\n",
    "    \"\"\"\n",
    "    Create response time analysis visualizations with clear labels and formatting.\n",
    "    \"\"\"\n",
    "    # set consistent colors and styles\n",
    "    colors = {\n",
    "        \"Basic Action Prompt\": \"blue\",\n",
    "        \"Enhanced Action Prompt + System Message\": \"brown\",\n",
    "    }\n",
    "    markers = {0: \"o\", 1: \"X\"}  # 0 for lost, 1 for won\n",
    "\n",
    "    # create figure with more space\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    ax1 = plt.subplot(121)\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"trial_group\",\n",
    "        y=\"average_response_time\",\n",
    "        hue=\"game_won\",\n",
    "        ax=ax1,\n",
    "        palette=[\"blue\", \"brown\"],\n",
    "    )\n",
    "    ax1.set_title(\"LLM Response Times by Trial and Game Outcome\\n(Box Plot)\", pad=10)\n",
    "    ax1.set_xlabel(\"Strategy Type\")\n",
    "    ax1.set_ylabel(\"Average Response Time (seconds)\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    ax1.legend(\n",
    "        title=\"Game Result\",\n",
    "        labels=[\"Lost\", \"Won\"],\n",
    "        handles=[\n",
    "            plt.Rectangle((0, 0), 1, 1, fc=\"blue\"),\n",
    "            plt.Rectangle((0, 0), 1, 1, fc=\"brown\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # scatter plot\n",
    "    ax2 = plt.subplot(122)\n",
    "\n",
    "    # plot each combination separately with explicit styles\n",
    "    for trial in [\"Basic Action Prompt\", \"Enhanced Action Prompt + System Message\"]:\n",
    "        for outcome in [0, 1]:  # 0 for lost, 1 for won\n",
    "            mask = (df[\"trial_group\"] == trial) & (df[\"game_won\"] == outcome)\n",
    "            ax2.scatter(\n",
    "                df[mask][\"average_response_time\"],\n",
    "                df[mask][\"num_turns\"],\n",
    "                c=colors[trial],\n",
    "                marker=markers[outcome],\n",
    "                label=f\"{trial.split()[0]} - {'Won' if outcome else 'Lost'}\",\n",
    "            )\n",
    "\n",
    "    ax2.set_title(\"Response Time vs Number of Turns\\n(Scatter Plot)\", pad=10)\n",
    "    ax2.set_xlabel(\"Average Response Time (seconds)\")\n",
    "    ax2.set_ylabel(\"Number of Turns\")\n",
    "\n",
    "    ax2.legend(title=\"Strategy and Outcome\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "response_time_fig = plot_response_time_analysis(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Time Analysis\n",
    "\n",
    "#### Left Plot: Response Time Distribution (Box Plot)\n",
    "This box plot shows how long the LLM takes to respond (in seconds) for each strategy:\n",
    "- The boxes show where most of the response times fall\n",
    "- The horizontal line in each box is the median response time\n",
    "- The whiskers show the full range (excluding outliers)\n",
    "- Used to compare response times between winning and losing games for each strategy\n",
    "\n",
    "Important points:\n",
    "- Longer/shorter boxes indicate more/less variation in response times\n",
    "- Different heights between boxes show if one strategy is generally faster/slower\n",
    "- Comparing \"Won\" vs \"Lost\" boxes shows if winning games had different response patterns\n",
    "\n",
    "#### Right Plot: Response Time vs Game Length (Scatter Plot)\n",
    "This scatter plot shows the relationship between how long the LLM takes to respond and how many turns the game lasts:\n",
    "- Each dot represents one game\n",
    "- Different colors show the two different strategies\n",
    "- Different shapes show whether the game was won or lost\n",
    "- Position shows both the response time (left-to-right) and number of turns (bottom-to-top)\n",
    "\n",
    "Important points:\n",
    "- Clustering of points shows common patterns\n",
    "- Spread of points shows how consistent the relationship is\n",
    "- Position of won/lost games shows if faster responses led to better outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
